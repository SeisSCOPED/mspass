<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>

    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <title>Data Object Design Concepts</title>
  </head>
  <body>
    <h1 align="center">Data Object Design Concepts</h1>
    <h3>Overview<br>
    </h3>
    <p>The core data objects in MsPASS were designed to encapsulate the
      most atomic objects in seismology waveform process:&nbsp; scalar
      (i.e. single channel) signals, and three component
      signals.&nbsp;&nbsp; The versions of these you as a user should
      normally interact with are two objects defined in MsPASS as <i>TimeSeries</i>
      and <i>Seismogram</i> respectively.&nbsp;&nbsp; <br>
    </p>
    <p>Data objects are grouped in memory with a generic concept called
      an <i>Ensemble</i>.&nbsp;&nbsp; The implementation in C++ uses a
      template to define a generic ensemble.&nbsp;&nbsp; A limitation of
      the current capability to link C++ binary code with python is that
      templates do not translate directly.&nbsp;&nbsp; Consequently, the
      python interface uses two different names to define Ensembles of
      TimeSeries and Seismogram objects:&nbsp; <i>TimeSeriesEnsemble</i>
      and <i>ThreeComponentEnsemble</i> respectively.<i> &nbsp;
        ThreeComponentEnemble </i>is a bit of a mismatch in a naming
      convention, but we felt the name was long enough already that the
      name seems clearer than the alternative of SeismogramEnsemble.
      &nbsp;<i> </i><br>
    </p>
    <h3>History</h3>
    <p>It might be helpful for the user to recognize that the core data
      objects in MsPASS are a second generation of a set of data objects
      developed by one of the authors (Pavlis) over a period of more
      than 15 years.&nbsp;&nbsp; The original implementation was
      developed as a component of Antelope through the open source add
      on to Antelope distributed through the <a
        href="antelopeusersgroup">antelope user's group</a> and referred
      to as SEISPP. &nbsp; The bulk of the original code can be found <a
href="https://github.com/antelopeusersgroup/antelope_contrib/tree/master/lib/seismic/libseispp">here</a>
      in github, and doxygen generated pages comparable to those found
      with this package can be found <a
        href="http://www.indiana.edu/%7Epavlab/software/seispp/html/index.html">here</a>.&nbsp;
      <br>
    </p>
    <p>To build the core data components from this older library we
      followed standard advice and mostly burned the original keeping
      only the most generic with features the authors had found useful
      over the years.&nbsp;&nbsp; The revisions followed these
      guidelines:<br>
    </p>
    <ul>
      <li>Make the API as generic as possible.</li>
      <li>Use inheritance more effectively to make the class structure
        more easily extended to encapsulate different variations in
        seismic data.</li>
      <li>Divorce the lower level API completely from Antelope to
        achieve the full open source goals of MsPASS.&nbsp; Although not
        implemented at this time, the concept is that an extension to
        work with Antelope data structures would require only a set of
        different class extensions. <br>
      </li>
      <li>Eliminate unnecessary/extraneous constructors and methods
        developed by the authors as the class structure evolved
        organically over the years.&nbsp; In short, think through the
        core concepts more carefully.<br>
      </li>
      <li>Extend the Metadata object (see below) to provide support for
        more types (objects) than the lowest common denominator of
        floats, ints, strings, and booleans.&nbsp;&nbsp; <br>
      </li>
    </ul>
    MsPASS has hooks to and leans heavily on <a
      href="https://github.com/obspy/obspy/wiki">obspy</a>.&nbsp;&nbsp;
    We chose, however, to diverge some from obspy in some fundamental
    ways.&nbsp;&nbsp; We found two fundamental flaws in obspy for large
    scale processing that required this divergence:<br>
    <ol>
      <li>obspy handles what we call Metadata through set of python
        objects that have to be maintained separately and we think
        unnecessarily complicate the API.&nbsp;&nbsp; We aimed instead
        to simplify the management of Metadata much as we could to make
        the system more like seismic reflection processing systems that
        manage the same problem through a fixed header
        namespace.&nbsp;&nbsp; We use a more flexible namespace, but
        users familiar with traditional reflection processing may find
        this a useful conceptual model.</li>
      <li>obspy does no handle three component data in a native way, but
        mixes up the concepts we call <i>Seismogram </i>and <i>Ensemble</i>
        in to a common python object they define as a <a
href="http://docs.obspy.org/packages/autogen/obspy.core.stream.Stream.html#obspy.core.stream.Stream">Stream</a>.&nbsp;&nbsp;
        We would argue our model is a more logical encapsulation of the
        concepts that define these ideas.&nbsp; <br>
      </li>
    </ol>
    <h3>Core Concepts</h3>
    <h4>Overview - Inheritance Relationships</h4>
    <p>The reader needs to see the big picture of how TimeSeries and
      Seismogram objects are defined to understand the core concepts
      described in sections that follow.&nbsp; We assume the reader has
      some understanding of the concepts of inheritance in object
      oriented code, but it has been our experience that there are a lot
      of more subtle issues that, in practice, are harder to articulate
      and are implementation dependent.&nbsp; Perhaps the best
      justification of the structure we used is that is is derived from
      SEISPP (see history above) and we aimed to rebranch and prune
      based on the experience we had with this approach from the
      development history of SEISPP.&nbsp;&nbsp; <br>
    </p>
    <p>The (admittedly) complicated inheritance diagrams for TimeSeries
      and Seismogram objects generated by doxygen are illustrated below
      <br>
      <img src="TimeSeriesInheritance.png" alt="TimeSeries Inheritance"
        height="58%" width="75%"><br>
      <img src="SeismogramInheritance.png" alt="Seismogram Inheritance"
        height="56%" width="75%"><br>
    </p>
    <p>Notice that both CoreSeismogram and CoreTime series have a common
      inheritance using multiple inheritance.&nbsp;&nbsp; Python does
      not currently support multiple inheritance so the python interface
      was designed to largely hide this detail from the user.&nbsp;
      CoreTimeSeries and CoreSeismogram should be though of a core data
      that is independent of MsPASS.&nbsp;&nbsp; Common features need by
      both objects to interact with MsPASS are inherited from
      MsPASSCoreTS.&nbsp;&nbsp;&nbsp; A key point is future users could
      chose to prune the MsPASSCoreTS component and build on
      CoreTimeSeries and CoreSeismogram and have no dependence upon
      MsPASS.&nbsp; <br>
    </p>
    <h4>BasicTimeSeries&lt;make a link when location is settled&gt; -
      Base class of common data characteristics <br>
    </h4>
    <p>This base class object can be best viewed as an answer to this
      questions:&nbsp; What is a time series?&nbsp;&nbsp; This class
      design answers this question as containing the following elements:</p>
    <ol>
      <li>We define a time series as data that has a<b> fixed sample
          rate.</b>&nbsp;&nbsp; Some extend this to arbitrary x-y data,
        but we view that as wrong.&nbsp; Standard textbooks on signal
        processing focus exclusively on uniformly sampled data.&nbsp;
        With that assumption the time of any sample is virtual and does
        not need to be stored.&nbsp; Hence, the base object has methods
        to convert sample numbers to time and the inverse (time to
        sample number).</li>
      <li>Data processing always requires the time series have a <b>finite
          length</b>.&nbsp;&nbsp; Hence, our definition of a time series
        directly supports windowed data of a specific length (public
        attribute <i>ns</i> - mnemonic abbreviation for number of
        samples).&nbsp; This definition does not preclude an extension
        to modern continuous data sets that are too large to fit in
        memory, but assume is an extension we don't currently
        support.&nbsp; e.g one way to implement an abstraction of an
        existing continuous channel of data is an
        std::list&lt;TimeSeries&gt;, although that would prove
        impractical for all but a short running instrument as the size
        could quickly overwhelm all but the largest memory machines.</li>
      <li>We assume the data has been cleaned and <b>lacks data gaps</b>.&nbsp;
        Real continuous data today nearly always have gaps at a range of
        scale created by a range of possible problems that create
        gaps:&nbsp; telemetry gaps, power failures, instrument failures.
        time tears, and with older data gaps created by station
        servicing.&nbsp; MsPASS has stubs API definitions for data with
        gaps, but there are currently no implementations and it is low
        on our development priority to do so.&nbsp;&nbsp; Since the main
        goal of MsPASS is to provide a framework for efficient
        processing of large data sets, we pass the job of finding and/or
        fixing data gaps to other packages or algorithms using MsPASS
        with a "when in doubt throw it out" approach to
        editing.&nbsp;&nbsp; <br>
      </li>
    </ol>
    <p>BasicTimeSeries uses public attributes to define the base
      properties discussed in the points above and has methods that are
      common to any data with these properties.&nbsp; (e.g. a time(n)
      method returns the computed time for sample number n.)&nbsp;&nbsp;
      An unusual attribute borrowed from reflection processing is the
      boolean variable with the name <i>live</i>.&nbsp;&nbsp; Data not
      marked live (live == false) should normally be passed through a
      processing chain, but will always be dropped by database
      writers.&nbsp; <br>
    </p>
    <h4>Handling Time</h4>
    <p>MsPASS uses a generalization to handle time that is the same as a
      novel method used in the original SEISPP library.&nbsp;&nbsp; The
      concept can be thought of as a generalized, but simplified version
      of how SAC handles time.&nbsp;&nbsp; The time standard is defined
      by an enum class in C++ called tref which is mapped to fixed names
      in python.&nbsp;&nbsp; There are currently two options:&nbsp; <br>
    </p>
    <ol>
      <li>When tref is TimeReferenceType::Relative
        (TimeReferenceType.Relative in python) the computed times are
        some relatively small number from some well defined time
        mark.&nbsp;&nbsp; The most common relative standard is the
        implicit time standard used in all seismic reflection
        data:&nbsp; shot time.&nbsp;&nbsp; SAC users will recognize this
        ideas as the case when IZTYPE==IO.&nbsp;&nbsp; Another important
        one used in MsPASS is an arrival time reference, which is a
        generalization of the case in SAC with IZTYPE==IA or ITn.&nbsp;
        We intentionally do not limit what this standard actually
        defines as how the data are handled depends only on the choice
        of UTC versus Relative.&nbsp; The ASSUMPTION is that if an
        algorithm needs to know the detail of "relative to what?" means,
        that detail will be defined in a Metadata attribute.<br>
      </li>
      <li>When tref is TimeReferenceType::UTC all times are assumed to
        be an absolute time standard defined by coordinated universal
        time (UTC). &nbsp; We follow the approach used in Antelope and
        store ALL times defined as UTC as <a
          href="https://en.wikipedia.org/wiki/Unix_time">unix epoch
          times.</a>&nbsp; We use this simple approach for two
        reasons:&nbsp; (1) storage (times can be stored as a simple
        double precision (64 bit float) field), and (2) efficiency
        (computing relative times is trivial compared to handling
        calendar data).&nbsp;&nbsp; This is in contrast to obspy which
        require ALL start times (t0 in mspass data objects) be defined
        by a python class they call <a
href="https://docs.obspy.org/packages/autogen/obspy.core.utcdatetime.UTCDateTime.html#obspy.core.utcdatetime.UTCDateTime">UTCDateTime</a>.&nbsp;
        Since MsPASS is linked to obspy we recommend you use the
        UTCDateTime class in python if you need to convert from epoch
        times to one of the calendar structures UTCDateTime can handle.
        <br>
      </li>
    </ol>
    A more concise summary of what these two time standard mean is
    this:&nbsp; active source data always use Relative time and
    earthquake data are always stored in raw form as UTC time stamps
    (e.g. see the SEED standard).<br>
    <br>
    BasicTimeSeries defines two methods to convert between these two
    time standards:&nbsp; rtoa (Relative to Absolute) and ator (Absolute
    to Relative).&nbsp; Be aware the library has internal checks to
    avoid an invalid conversion from relative to absolute with the
    rtoa() method.&nbsp; This was done to avoid errors from trying to
    convert active source data to an absolute time standard when the
    true time is not well constrained.&nbsp; <br>
    <h4>Metadata and MetadataDefinitions</h4>
    <p>All data objects used by the MsPASS C++ library inherit a
      Metadata object.&nbsp; A <i>Metadata</i> object is best thought
      of through either of two concepts well known to most
      seismologists:&nbsp; (1) headers (SAC), and (2) a dictionary
      container in python.&nbsp;&nbsp; Both are ways to handle a
      general, modern concept of <a
        href="https://en.wikipedia.org/wiki/Metadata">metadata</a>
      commonly defined as "data that provides information about
      data".&nbsp; Packages like SAC use fixed (usually binary fields)
      slots in an external data format to define a finite set of
      attributes with a fixed namespace.&nbsp;&nbsp; obspy uses a python
      dictionary like container they call <a
href="https://docs.obspy.org/packages/autogen/obspy.core.trace.Stats.html">Stats</a>
      to store comparable information.&nbsp;&nbsp; That approach allows
      metadata attributes to be extracted from a flexible container
      addressable by a key word and that can contain any valid
      data.&nbsp;&nbsp; For example, a typical obspy script will contain
      a line like this:<br>
    </p>
    <p>sta=d.Stats["station]<br>
    </p>
    <p>to fetch the station name from a Trace object, d.&nbsp; <br>
    </p>
    <p>In MsPASS we use a similar concept building on Pavlis's SEISPP
      library developed originally a number of years before
      obspy.&nbsp;&nbsp; The Metadata object in MsPASS, however, has
      additional features not in the older SEISPP version.&nbsp;&nbsp; <br>
    </p>
    <p>The mspass::Metadata object has a container that can hold any
      valid data much like a python dictionary.&nbsp;&nbsp; The current
      implementation uses the <a
        href="https://theboostcpplibraries.com/boost.any">any</a>
      library that is part of the widely used boost library.&nbsp;&nbsp;
      In a C++ program Metadata can contain any data that, to quote the
      documentation, is "copy constructable".&nbsp; The python
      interface, however, is much more restrictive for a number of
      reasons.&nbsp; The most important, however, is that to interact
      cleanly with MongoDB we elected to limit the set of allowed types
      for Metadata attributes to those supported as distinct types in
      the python MongoDB API.&nbsp;&nbsp; That list is defined <a
        href="https://docs.mongodb.com/manual/reference/bson-types/">here</a>.&nbsp;
      In principle, MongoDB can support generic "array" and "object"
      types that could contain serialized containers, but currently
      MsPASS only supports core types in all database engines:&nbsp;
      real numbers (float or double), integers (32 or 64 bit), strings
      (currently assumed to be UTF-8), and booleans.&nbsp;&nbsp; This
      creates some rigidity in the python API to a Metadata.&nbsp;&nbsp;
      There are four "getters" seen in the following contrived code
      segment:<br>
    </p>
    <blockquote># Assume d is a Seismogram or TimeSeries which
      automatically casts to a Metadata in the python API use here<br>
      x=d.get_double("t0")&nbsp;&nbsp;&nbsp; # example fetching a
      floating point number - here a start time<br>
      n=d.get_int("nsamp")&nbsp;&nbsp; # example fetching an integer<br>
      s=d.get_string("sta")&nbsp;&nbsp;&nbsp; # example fetching a UTF-8
      string<br>
      b=d.get_bool("LPSPOL")&nbsp; # boolean for positive polarity used
      in SAC<br>
    </blockquote>
    The putters are overloaded methods, which is not a standard concept
    in python.&nbsp;&nbsp; For those not familiar with overloading it
    means which function to call is sorted out by the type of the
    arguments.&nbsp; To demonstrate the following code snippet is the
    inverse of the getters above (type of the arguments is defined
    above)<br>
    <blockquote>d.put("t0",x)<br>
      d.put("nsamp",n)<br>
      d.put("sta",sta)<br>
      d.put("LPSPOL",bool(1))&nbsp; <br>
    </blockquote>
    The odd construct for boolean is necessary because of a collision
    between C and python demonstrated in this example.&nbsp; In C a
    boolean could, in principle, be defined by one bit (in all
    implementations the actual test is 0 versus nonzero and the storage
    is the native integer size).&nbsp;&nbsp; In python a boolean is a
    class (object).&nbsp; The bool(1) construct provides that
    interface.&nbsp;&nbsp; This is a potential confusion in writing
    python scripts because a python boolean <br>
    <p>Mapping the C++ Metadata container to python was a challenge
      because of a fundamental difference in an axiom of the two
      languages:&nbsp;&nbsp; python has a loose definition of "type"
      while C/C++ are "strongly typed".&nbsp;&nbsp; To understand the
      difference note that all C/C++ code REQUIRES all variables to be
      declared before use with a type specification while python has no
      concept of "declaration" in the language at all.&nbsp; In python
      the same variable name can change from a simple integer to some
      much more complicated type like an obspy Trace object.&nbsp;
      Similar usage in a C program will always fail to
      compile.&nbsp;&nbsp; To assure consistency on this issue the
      Metadata container will throw an exception if a user tries to
      extract a parameter (call one of the getters) with the wrong
      type.&nbsp;&nbsp; For example:<br>
    </p>
    <blockquote>d.put("sta","AAK")<br>
      s=d.get_string("sta")&nbsp;&nbsp; # this succeeds because sta was
      set a string<br>
      x-d.get_double("sta")&nbsp; # this will throw an exception because
      "sta" was not set as a real number.<br>
    </blockquote>
    This effectively creates a strong typing layer between python and
    the C libraries to prevent type collisions that would otherwise be
    too easy to create.&nbsp;&nbsp; A related feature in MsPASS
    described in the next section, which we call MetadataDefinitions,
    can be thought of as a referee that can be used to guarantee type
    consistency of any key:value pair that is to be read from or written
    to MongoDB.&nbsp; <br>
    <h4>MetadataDefinitions object</h4>
    <p>A MetadataDefinitions object is required by all MongoDB functions
      that perform CRUD operations to MongoDB with data
      objects.&nbsp;&nbsp; It has two critical purposes when interacting
      with MongoDB:<br>
    </p>
    <ol>
      <li>It manages type properties and enforces decisions about
        whether a Metadata attribute is mutable in writes and
        updates.&nbsp; A typical example would be station properties
        like the location of the sensor, instrument response data,
        etc.&nbsp; Such parameters are expected to be read once by a
        reader and passed through a processing workflow until a write
        operation.&nbsp; They also are normally expected to be <a
          href="https://docs.mongodb.com/manual/core/data-model-design/">normalized</a>
        with the master copy in a separate collection from waveform
        data.&nbsp;&nbsp; <br>
      </li>
      <li>It is used by readers to sort out potentially ambiguous
        keys.&nbsp; A typical example would be instrument
        characteristics of a seismic observatory station.&nbsp;&nbsp;
        Sensors are changed, channel codes are changed, sensors can
        change orientation when swapped, etc.&nbsp;&nbsp; This can make
        critical metadata like response information time
        variable.&nbsp;&nbsp; (e.g. asking for the response data for
        station AAK channel BHZ is ambiguous for multiple
        reasons.)&nbsp;&nbsp; MetadataDefinitions was designed to
        abstract such information and front load the process of
        resolving such ambiguities to readers.&nbsp;&nbsp; More details
        on this interaction are given in the description (WILL NEED A
        LINK HERE) of the MongoDB python API.&nbsp;&nbsp;&nbsp;</li>
    </ol>
    For most users the practical issue is that most processing workflows
    will need to include these lines near the top of any python script:<br>
    <blockquote>from mspasspy import MetadataDefinitions<br>
      mdef=MetadataDefinitions()<br>
    </blockquote>
    This loads the default namespace.&nbsp;&nbsp; Alternatives are
    possible, but should be used only for specialized applications
    algorithms that require a different namespace.&nbsp; For example, in
    principle it should be possible to build a specialized configuration
    to build a MetadataDefinitions object that could be used to
    translate between the SAC or SEGY namespaces and mspass.
    &nbsp;&nbsp; <br>
    <h4>Scalar versus 3C data</h4>
    <p>MsPASS currently supports two different data objects:&nbsp;&nbsp;
      TimeSeries is used to store single channel data while Seismogram
      is used store data from three component instruments.&nbsp;
      TimeSeries objects are based on the standard concept for storing
      scalar data that has been around since the earliest days of
      digital seismic data in the oil and gas industry.&nbsp; That is,
      the sample values are stored in a continuous
      array/vector.&nbsp;&nbsp; The index for the array serves as a
      proxy for time.&nbsp;&nbsp; We use a C++ <a
        href="http://www.cplusplus.com/reference/vector/vector/">standard
        template library vector container</a> to hold the sample data
      accessible through the public variable s.&nbsp; The python API
      makes the vector container look like a numpy array that can be
      accessed in same way sample data are handled in an obspy Trace
      object in the "data" array.&nbsp;&nbsp; <br>
    </p>
    <p>Although scalar time series data are treated the same (i.e. as a
      vector) in every seismic processing system we are aware of, the
      handling of three component data is not at all
      standardized.&nbsp;&nbsp; There are several reasons for this
      created by some practical data issues: <br>
    </p>
    <ol>
      <li>&nbsp;Most modern seismic reflection systems provide some
        support for three-component data.&nbsp;&nbsp; Scalar,
        multichannel raw data are conceptually treated as a matrix with
        on array dimension defining the time variable and the other
        index defined by the channel number. When three component data
        are recorded the component orientation can be defined implicitly
        by a component index number.&nbsp;&nbsp; A shot gather than can
        be indexed conveniently with three array indexes.&nbsp; A
        complication in that approach is that which index is used for
        which of the three concept required for a gather of 3C
        data&nbsp; is completely undefined.&nbsp;&nbsp; Furthermore, for
        a generic system like mspass the multichannel model does not map
        cleanly into passive array data because a collection of 3C
        seismograms have irregular size, may have variable sample
        rates,&nbsp; and variable instrumentation.&nbsp; Hence, a simple
        matrix or array model would be very limiting.</li>
      <li>Traditional multichannel data have synchronous time
        sampling.&nbsp;&nbsp; Seismic reflection processing always
        assumes during processing that time computed from sample numbers
        is accurate to within one sample.&nbsp;&nbsp; That is not
        necessarily true in passive array data and raw processing
        requires efforts to make sure the time of all samples can be
        computed accurately and time aligned.&nbsp; Alignment for a
        single stations is normally automatic although some instruments
        have measurable, constant phase lags at the single sample
        level.&nbsp; The bigger issue for all modern data is that the
        raw data are rarely stored in a multiplexed multichannel format,
        although the SEED format allows that. &nbsp; Most passive array
        data streams have multiple channels stored as compressed
        miniSEED packets that have to be unpacked and inserted into
        something like a vector container to be handled easily by a
        processing program.&nbsp;&nbsp; The process becomes more
        complicated for three-component data because at least three
        channels have to be manipulated and time aligned.&nbsp;&nbsp;
        The obspy package handles this issue by defining a Stream object
        that is a container of single channel Trace objects.&nbsp; They
        handle three component data as Stream objects with exactly three
        members in the container.&nbsp;&nbsp; <br>
      </li>
    </ol>
    We handle three component data in MsPASS by using a matrix, which we
    define with the symbol "u" following the convention in Aki and
    Richards, to store the data.&nbsp;&nbsp; There are two choices of
    the order of indices for this matrix.&nbsp; A Seismogram has defines
    index 1 as the channel number and index 2 as the time index.&nbsp;
    The following python code section illustrates this more clearly than
    any words:<br>
    <blockquote>from mspasspy import Seismogram<br>
      d=Seismogram(100)&nbsp;&nbsp;&nbsp;&nbsp; # Create an empty
      Seismogram with storage for 100 time steps initialized to all
      zeros<br>
      d.u(0,50)=1.0
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      # Create a delta function at time t0+dt*50 in channel 0<br>
    </blockquote>
    Note we use the C convention for indexing starting at 0.&nbsp;&nbsp;
    In the C++ API the matrix u is defined with a lightweight
    implementation of a matrix as object.&nbsp;&nbsp; That detail is
    largely irrelevant to python programmers as the matrix is
    equivalenced to a numpy matrix.&nbsp;&nbsp; Hence, python
    programmers familiar with numpy can manipulate the data in the u
    matrix with all the tools of numpy.&nbsp; <br>
    <br>
    The Seismogram object has a minimal set of methods that the authors
    consider core concepts defining a three component seismogram.&nbsp;
    We limit these to coordinate transformations of the
    components.&nbsp;&nbsp; There are multiple methods for rotation of
    the components (overloaded rotate method), restoring data to
    cardinal directions at the instrument (rotate_to_standard),
    Kennett's free surface transformation, and a general transformation
    matrix.&nbsp;&nbsp; We use a pair of boolean variables that are
    helpful for efficiency:&nbsp; <i>components_are_orthogonal</i> is
    true after any sequence of orthogonal transformations and <i>components_are_cardinal</i>
    is true when the components are in the standard ENZ directions.
    &nbsp; &nbsp; <br>
    <br>
    FIX BEFORE RELEASE:&nbsp;&nbsp; ENSEMBLE WRAPPERS HAVE NOT YET BEEN
    DEFINED OR TESTED<br>
    Ensembles of TimeSeries and Seismogram data are handled with a more
    elaborate standard template library container.&nbsp;&nbsp; For
    readers familiar with C++ the generic definition of an Ensemble is
    the following class definition created by stripping the comments
    from the definition in Ensemble.h:<br>
    <blockquote>template &lt;typename Tdata&gt; class Ensemble : public
      Metadata<br>
      {<br>
      public:<br>
      &nbsp; vector&lt;Tdata&gt; member;<br>
      &nbsp; ...<br>
      &nbsp; Tdata&amp; operator[](const int n) const<br>
      &nbsp; ...<br>
      }<br>
    </blockquote>
    where we omit all standard constuctors and methods to focus on the
    key issues here.&nbsp; First, an Ensemble is little more than a
    vector of data objects with a Metadata object to store attributes
    common to the entire ensemble.&nbsp; Hence, the idea is to store
    global attributes in the Ensemble Metadata field.&nbsp;&nbsp; There
    is a "dismember" algorithm in MsPASS (NOT YET IMPLEMENTED by already
    present in seispp and easy to implement) that takes this structure
    apart and copies the Metadata components into each member.&nbsp; The
    vector container makes it simple to handle an entire group
    (Ensemble) with a simple loop.&nbsp;&nbsp; e.g. here is a simple
    loop to work through an entire Ensemble (defined in this code
    segment with the symbol d) in order of the vector index:<br>
    <blockquote>n=d.member.size()<br>
      for i in range(n):<br>
      &nbsp; somefunction(d.member[i])&nbsp;&nbsp;&nbsp; # pass member i
      to somefunction<br>
    </blockquote>
    NOT SURE ABOUT THE ABOVE SYNTAX IN THE PYTHON API.&nbsp;&nbsp; COULD
    BE () INSTEAD OF []<br>
    <h4>MsPASSCoreTS&nbsp; and Core versus Top-level Data Objects<br>
    </h4>
    <p>The class hierarchy diagrams above show there are CoreTimeSeries
      and CoreSeismogram objects that are parents of TimeSeries and
      Seismogram respectively.&nbsp;&nbsp; That design was aimed to make
      the Core objects more readily extendible to other uses than
      MsPASS.&nbsp;&nbsp; We encourage users to consider using the core
      objects as base for other ways of handling seismic
      data.&nbsp;&nbsp; <br>
    </p>
    <p>All mspass specific elements of our implementation are in
      MsPASSCoreTS which is a parent for both TimeSeries and Seismogram
      objects.&nbsp;&nbsp; MsPASSCoreTS has two elements:<br>
    </p>
    <ol>
      <li>In MsPASS we use MongoDB for data management.&nbsp;&nbsp; In
        MongoDB the lowest common denominator to identify a particular
        "document" in the database is the <a
          href="https://docs.mongodb.com/manual/reference/method/ObjectId/">ObjectID</a>.&nbsp;
        We store a representation of the ObjectID that was used to
        create any data object read from the database.&nbsp;&nbsp;
        Whenever any processing algorithm alters the sample in an
        object, however, the ObjectID is set invalid.&nbsp;&nbsp;&nbsp;
        Database writers use an invalid ObjectID as a signal that an
        image of such data needs to be saved.&nbsp; If, however, the
        ObjectID is valid a writer assumes it only needs to do a
        database update to Metadata attributes.&nbsp; Metadata has a
        method to return the keys of all Metadata attributes that were
        changed since the original creation from an earlier read.&nbsp;
        That feature is important for efficiency in pure Metadata
        operations or processing that can be reduce to a new set of
        Metadata. <br>
      </li>
      <li>MsPASSCoreTS contains an error logging object.&nbsp;&nbsp; The
        purpose of this object is to contain a log of any errors or
        informative messages created during the processing of the
        data.&nbsp; All processing modules need to be designed with
        global error handlers so that they never abort, but in worst
        case post a log message that tags a fatal error.&nbsp;&nbsp;
        More details on this feature are given in the next section. <br>
      </li>
    </ol>
    <h4>Error Logging Concepts<br>
    </h4>
  </body>
</html>
