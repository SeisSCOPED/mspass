<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <title>Data Object Design Concepts</title>
  </head>
  <body>
    <h1 align="center">Data Object Design Concepts</h1>
    <h3>Overview<br>
    </h3>
    <p>The core data objects in MsPASS were designed to encapsulate the
      most atomic objects in seismology waveform process:&nbsp; scalar
      (i.e. single channel) signals, and three component
      signals.&nbsp;&nbsp; The versions of these you as a user should
      normally interact with are two objects defined in MsPASS as <i>TimeSeries</i>
      and <i>Seismogram</i> respectively.&nbsp;&nbsp; <br>
    </p>
    <p>These data objects were designed to simply interactions with
      MongoDB.&nbsp; MongoDB is completely flexible in attributes names
      handled by the database.&nbsp; We manage the attribute names and
      data types through an interfacing object we call <i>MetadataDefinitions</i>
      that is a required in most python database interactions.<br>
    </p>
    <p>Data objects are grouped in memory with a generic concept called
      an <i>Ensemble</i>.&nbsp;&nbsp; The implementation in C++ uses a
      template to define a generic ensemble.&nbsp;&nbsp; A limitation of
      the current capability to link C++ binary code with python is that
      templates do not translate directly.&nbsp;&nbsp; Consequently, the
      python interface uses two different names to define Ensembles of
      TimeSeries and Seismogram objects:&nbsp; <i>TimeSeriesEnsemble</i>
      and <i>ThreeComponentEnsemble</i> respectively.<i> &nbsp;
        ThreeComponentEnemble </i>is a bit of a mismatch in a naming
      convention, but we felt the name was long enough already that the
      name seems clearer than the alternative of
      SeismogramEnsemble.&nbsp; <br>
    </p>
    <p>The C++ objects have wrappers for python that hide the details
      from the user. &nbsp; All MongoDB operations implemented with the
      pymongo package using these wrappers. &nbsp; Numerical operations
      on the sample data should either be written in C/C++ with their
      own wrappers or exploit numpy/scipy numerical routines. &nbsp; The
      later is possible because the wrappers make the data arrays look
      like numpy arrays. &nbsp;<i> </i><br>
    </p>
    <h3>History</h3>
    <p>It might be helpful for the user to recognize that the core data
      objects in MsPASS are the second generation of a set of data
      objects developed by one of the authors (Pavlis) over a period of
      more than 15 years.&nbsp;&nbsp; The original implementation was
      developed as a component of Antelope.&nbsp; It was distribured via
      the open source additions to Antelope distributed through the <a
        href="antelopeusersgroup">antelope user's group</a> and referred
      to as SEISPP. &nbsp; The bulk of the original code can be found <a
href="https://github.com/antelopeusersgroup/antelope_contrib/tree/master/lib/seismic/libseispp">here</a>
      in github, and doxygen generated pages comparable to those found
      with this package can be found <a
        href="http://www.indiana.edu/%7Epavlab/software/seispp/html/index.html">here</a>.&nbsp;

      <br>
    </p>
    <p>To build the core data objects from this older library we
      followed standard advice and mostly burned the original keeping
      only the most generic with features the authors had found
      essential over the years.&nbsp;&nbsp; The revisions followed these
      guidelines:<br>
    </p>
    <ul>
      <li>Make the API as generic as possible.</li>
      <li>Use inheritance more effectively to make the class structure
        more easily extended to encapsulate different variations in
        seismic data.</li>
      <li>Divorce the API completely from Antelope to achieve the full
        open source goals of MsPASS.&nbsp; Although not implemented at
        this time, the design will allow a version 2 of SEISPP in
        Antelope, although that is a "small matter of programming" that
        may never happen. <br>
      </li>
      <li>Eliminate unnecessary/extraneous constructors and methods
        developed by the authors as the class structure evolved
        organically over the years.&nbsp; In short, think through the
        core concepts more carefully and treat SEISPP as a prototype.<br>
      </li>
      <li>Extend the Metadata object (see below) to provide support for
        more types (objects) than the lowest common denominator of
        floats, ints, strings, and booleans handled by
        SEISPP.&nbsp;&nbsp; <br>
      </li>
    </ul>
    MsPASS has hooks to and leans heavily on <a
      href="https://github.com/obspy/obspy/wiki">obspy</a>.&nbsp;&nbsp;
    We chose, however, to diverge some from obspy in some fundamental
    ways.&nbsp;&nbsp; We found two fundamental flaws in obspy for large
    scale processing that required this divergence:<br>
    <ol>
      <li>obspy handles what we call Metadata through set of python
        objects that have to be maintained separately and we think
        unnecessarily complicate the API.&nbsp;&nbsp; We aimed instead
        to simplify the management of Metadata much as possible.&nbsp;
        Our goal was to make the system more like seismic reflection
        processing systems that manage the same problem through a simple
        namespace wherein metadata can be fetched with simple
        keys.&nbsp;&nbsp; We aimed to hide any hierarchic structures
        (e.g relational database table relationships, obspy object
        hierarchies,&nbsp; or MongoDB normalized data structures) behind
        the <i>MetadataDefinitions </i>object to reduce all Metadata
        to pure name:value pairs.&nbsp; <br>
      </li>
      <li>obspy does not handle three component data in a native way,
        but mixes up the concepts we call <i>Seismogram </i>and <i>Ensemble</i>
        in to a common python object they define as a <a
href="http://docs.obspy.org/packages/autogen/obspy.core.stream.Stream.html#obspy.core.stream.Stream">Stream</a>.&nbsp;&nbsp;

        We would argue our model is a more logical encapsulation of the
        concepts that define these ideas.&nbsp; <br>
      </li>
    </ol>
    <h3>Core Concepts</h3>
    <h4>Overview - Inheritance Relationships</h4>
    <p>The reader needs to first see the big picture of how TimeSeries
      and Seismogram objects are defined to understand the core concepts
      described in sections that follow.&nbsp; We assume the reader has
      some understanding of the concepts of inheritance in object
      oriented code.&nbsp;&nbsp; The inheritance structure use we best
      understood as derived from the SEISPP prototype (see history
      above).&nbsp; We aimed to rebranch and prune
      SEISPP&nbsp; based on the experience from 15 years of development
      for SEISPP. <br>
    </p>
    <p>The (admittedly) complicated inheritance diagrams for TimeSeries
      and Seismogram objects generated by doxygen are illustrated below
      <br>
      <img src="TimeSeriesInheritance.png" alt="TimeSeries Inheritance"
        height="58%" width="75%"><br>
      <img src="SeismogramInheritance.png" alt="Seismogram Inheritance"
        height="56%" width="75%"><br>
    </p>
    <p>Notice that both CoreSeismogram and CoreTime series have a common
      inheritance from three base classes:&nbsp; <i>BasicTimeSeries,
        BasicMetadata, </i>and <i>MsPASSCoreTS</i>.&nbsp;&nbsp; Python
      supports multiple inheritance and the wrappers make dynamic
      casting within the hierarchy automatic.&nbsp; e.g. a<i> Seismogram</i>
      object can be passed directly to a python function that does only
      Metadata operations and it will be handled seamlessly because
      python does no enforce type signatures on functions.&nbsp;
      CoreTimeSeries and CoreSeismogram should be though of a core data
      that is independent of MsPASS.&nbsp;&nbsp; Common features need by
      both objects to interact with MsPASS are inherited from
      MsPASSCoreTS.&nbsp;&nbsp;&nbsp; A key point is that future users
      could chose to prune the MsPASSCoreTS component and build on
      CoreTimeSeries and CoreSeismogram and have no dependence upon
      MsPASS to build a completely different framework.&nbsp; <br>
    </p>
    <p>The remainder of this section discusses the individual components
      in the class hierarchy.<br>
    </p>
    <h4>BasicTimeSeries&lt;make a link when location is settled&gt; -
      Base class of common data characteristics <br>
    </h4>
    <p>This base class object can be best viewed as an answer to this
      questions:&nbsp; What is a time series?&nbsp;&nbsp; Our design
      answers this question by saying all time series data have the
      following elements:</p>
    <ol>
      <li>We define a time series as data that has a<b> fixed sample
          rate.</b>&nbsp;&nbsp; Some extend this to arbitrary x-y data,
        but we view that as wrong.&nbsp; Standard textbooks on signal
        processing focus exclusively on uniformly sampled data.&nbsp;
        With that assumption the time of any sample is virtual and does
        not need to be stored.&nbsp; Hence, the base object has methods
        to convert sample numbers to time and the inverse (time to
        sample number).</li>
      <li>Data processing always requires the time series have a <b>finite

          length</b>.&nbsp;&nbsp; Hence, our definition of a time series
        directly supports windowed data of a specific length (public
        attribute <i>ns</i> - mnemonic abbreviation for number of
        samples).&nbsp; This definition does not preclude an extension
        to modern continuous data sets that are too large to fit in
        memory, but that is an extension we don't currently
        support.&nbsp; <br>
      </li>
      <li>We assume the data has been cleaned and <b>lacks data gaps</b>.&nbsp;

        Real continuous data today nearly always have gaps at a range of
        scale created by a range of possible problems that create
        gaps:&nbsp; telemetry gaps, power failures, instrument failures,
        time tears, and with older data gaps created by station
        servicing.&nbsp; MsPASS has stub API definitions for data with
        gaps, but these are currently not implementations.&nbsp;&nbsp;
        Since the main goal of MsPASS is to provide a framework for
        efficient processing of large data sets, we pass the job of
        finding and/or fixing data gaps to other packages or algorithms
        using MsPASS with a "when in doubt throw it out" approach to
        editing.&nbsp;&nbsp; The machinery to handle gap processing
        exists in both obpsy and Antelope and provide possible path to
        solution for users needing more extensive gap processing
        functionality.<br>
      </li>
    </ol>
    <p>BasicTimeSeries uses public attributes to define the base
      properties discussed in the points above and has methods that are
      common to any data with these properties.&nbsp; (e.g. a time(n)
      method returns the computed time for sample number n.)&nbsp;&nbsp;
      An unusual attribute borrowed from reflection processing is the
      boolean variable with the name <i>live</i>.&nbsp;&nbsp; Data not
      marked live (live == false) should normally be passed through a
      processing chain, but will always be dropped by database
      writers.&nbsp; Other public attributes are public for convenience,
      but changing any of them must be done with caution.&nbsp;&nbsp; <br>
    </p>
    <h4>Handling Time</h4>
    <p>MsPASS uses a generalization to handle time that is the same as a
      novel method used in the original SEISPP library.&nbsp;&nbsp; The
      concept can be thought of as a generalized, but yet simplified
      version of how SAC handles time.&nbsp;&nbsp; The time standard is
      defined by an enum class in C++ called tref which is mapped to
      fixed names in python.&nbsp;&nbsp; There are currently two
      options:&nbsp; <br>
    </p>
    <ol>
      <li>When tref is TimeReferenceType::Relative
        (TimeReferenceType.Relative in python) the computed times are
        some relatively small number from some well defined time
        mark.&nbsp;&nbsp; The most common relative standard is the
        implicit time standard used in all seismic reflection
        data:&nbsp; shot time.&nbsp;&nbsp; SAC users will recognize this
        ideas as the case when IZTYPE==IO.&nbsp;&nbsp; Another important
        one used in MsPASS is an arrival time reference, which is a
        generalization of the case in SAC with IZTYPE==IA or ITn.&nbsp;
        We intentionally do not limit what this standard actually
        defines as how the data are handled depends only on the choice
        of UTC versus Relative.&nbsp; The ASSUMPTION is that if an
        algorithm needs to know the detail of "relative to what?" means,
        that detail will be defined in a Metadata attribute.<br>
      </li>
      <li>When tref is TimeReferenceType::UTC (TimeReferenceType.UTC in
        python) all times are assumed to be an absolute time standard
        defined by coordinated universal time (UTC). &nbsp; We follow
        the approach used in Antelope and store ALL times defined as UTC
        with <a href="https://en.wikipedia.org/wiki/Unix_time">unix
          epoch times.</a>&nbsp; We use this simple approach for two
        reasons:&nbsp; (1) storage (times can be stored as a simple
        double precision (64 bit float) field), and (2) efficiency
        (computing relative times is trivial compared to handling
        calendar data).&nbsp;&nbsp; This is in contrast to obspy which
        require ALL start times (t0 in mspass data objects) be defined
        by a python class they call <a
href="https://docs.obspy.org/packages/autogen/obspy.core.utcdatetime.UTCDateTime.html#obspy.core.utcdatetime.UTCDateTime">UTCDateTime</a>.&nbsp;

        Since MsPASS is linked to obspy we recommend you use the
        UTCDateTime class in python if you need to convert from epoch
        times to one of the calendar structures UTCDateTime can handle.
        <br>
      </li>
    </ol>
    A more concise summary of what these two time standard mean is
    this:&nbsp; active source data always use Relative time and
    earthquake data are always stored in raw form as UTC time stamps
    (e.g. see the SEED standard).&nbsp; UTC is a fixed standard while
    Relative could have other meanings.<br>
    <br>
    BasicTimeSeries defines two methods to convert between these two
    time standards:&nbsp; rtoa (Relative to Absolute) and ator (Absolute
    to Relative).&nbsp; Be aware the library has internal checks to
    avoid an invalid conversion from relative to absolute with the
    rtoa() method.&nbsp; This was done to avoid errors from trying to
    convert active source data to an absolute time standard when the
    true time is not well constrained.&nbsp; <br>
    <h4>Metadata and MetadataDefinitions</h4>
    <p>All data objects used by the MsPASS C++ library inherit a
      Metadata object.&nbsp; A <i>Metadata</i> object is best thought
      of through either of two concepts well known to most
      seismologists:&nbsp; (1) headers (SAC), and (2) a dictionary
      container in python.&nbsp;&nbsp; Both are ways to handle a
      general, modern concept of <a
        href="https://en.wikipedia.org/wiki/Metadata">metadata</a>
      commonly defined as "data that provides information about
      data".&nbsp; Packages like SAC use fixed (usually binary fields)
      slots in an external data format to define a finite set of
      attributes with a fixed namespace.&nbsp;&nbsp; obspy uses a python
      dictionary like container they call <a
href="https://docs.obspy.org/packages/autogen/obspy.core.trace.Stats.html">Stats</a>
      to store comparable information.&nbsp;&nbsp; That approach allows
      metadata attributes to be extracted from a flexible container
      addressable by a key word and that can contain any valid
      data.&nbsp;&nbsp; For example, a typical obspy script will contain
      a line like this:<br>
    </p>
    <p>sta=d.Stats["station]<br>
    </p>
    <p>to fetch the station name from a Trace object, d.&nbsp; <br>
    </p>
    <p>In MsPASS we use a similar concept building on Pavlis's SEISPP
      library developed originally a number of years before
      obspy.&nbsp;&nbsp; The Metadata object in MsPASS, however, has
      additional features not in the older SEISPP version.&nbsp;&nbsp; <br>
    </p>
    <p>The mspass::Metadata object has a container that can hold any
      valid data much like a python dictionary.&nbsp;&nbsp; The current
      implementation uses the <a
        href="https://theboostcpplibraries.com/boost.any">any</a>
      library that is part of the widely used boost library.&nbsp;&nbsp;
      In a C++ program Metadata can contain any data that, to quote the
      documentation, is "copy constructable".&nbsp; The python
      interface, however, is much more restrictive for a number of
      reasons.&nbsp; The most important, however, is that to interact
      cleanly with MongoDB we elected to limit the set of allowed types
      for Metadata attributes to those supported as distinct types in
      the python MongoDB API.&nbsp;&nbsp; That list is defined <a
        href="https://docs.mongodb.com/manual/reference/bson-types/">here</a>.&nbsp;

      In principle, MongoDB can support generic "array" and "object"
      types that could contain serialized containers, but currently
      MsPASS only supports core types in all database engines:&nbsp;
      real numbers (float or double), integers (32 or 64 bit), strings
      (currently assumed to be UTF-8), and booleans.&nbsp;&nbsp; This
      creates some rigidity in the python API to a Metadata.&nbsp;&nbsp;
      There are four "getters" seen in the following contrived code
      segment:<br>
    </p>
    <blockquote># Assume d is a Seismogram or TimeSeries which
      automatically casts to a Metadata in the python API use here<br>
      x=d.get_double("t0")&nbsp;&nbsp;&nbsp; # example fetching a
      floating point number - here a start time<br>
      n=d.get_int("nsamp")&nbsp;&nbsp; # example fetching an integer<br>
      s=d.get_string("sta")&nbsp;&nbsp;&nbsp; # example fetching a UTF-8
      string<br>
      b=d.get_bool("LPSPOL")&nbsp; # boolean for positive polarity used
      in SAC<br>
    </blockquote>
    There are parallel "putter":<br>
    <blockquote>d.put_double("to",x)<br>
      d.put_int("nsamp",n)<br>
      d.put_string("sta",s)<br>
      d.put_bool("LPSPOL",True)<br>
    </blockquote>
    <blockquote> </blockquote>
    <blockquote> </blockquote>
    <p>Mapping the C++ Metadata container to python was a challenge
      because of a fundamental difference in an axiom of the two
      languages:&nbsp;&nbsp; python has a loose definition of "type"
      while C/C++ are "strongly typed".&nbsp;&nbsp; To understand the
      difference note that all C/C++ code REQUIRES all variables to be
      declared before use with a type specification while python has no
      concept of "declaration" in the language at all.&nbsp; In python
      the same variable name can change from a simple integer to some
      much more complicated type like an obspy Trace object.&nbsp;
      Similar usage in a C program will always fail to
      compile.&nbsp;&nbsp; To assure consistency on this issue the
      Metadata container will throw an exception (RuntimeError in python
      and MsPASSError in C++) if a user tries to extract a parameter
      with the wrong type.&nbsp;&nbsp; For example:<br>
    </p>
    <blockquote>d.put("sta","AAK")<br>
      s=d.get_string("sta")&nbsp;&nbsp; # this succeeds because sta was
      set a string<br>
      x-d.get_double("sta")&nbsp; # this will throw an exception because
      "sta" was not set as a real number.<br>
    </blockquote>
    This effectively creates a strong typing layer between python and
    the C libraries to prevent type collisions that would otherwise be
    too easy to create.&nbsp;&nbsp; A related feature in MsPASS
    described in the next section, which we call MetadataDefinitions,
    can be thought of as a referee that can be used to guarantee type
    consistency of any key:value pair that is to be read from or written
    to MongoDB.&nbsp; <br>
    <h4>MetadataDefinitions&nbsp; and MongoDBConverter objects</h4>
    <p>A MetadataDefinitions object is required by all MongoDB functions
      that perform CRUD operations to MongoDB with data
      objects.&nbsp;&nbsp; It has two critical purposes when interacting
      with MongoDB: <br>
    </p>
    <ol>
      <li>It manages type properties and enforces decisions about
        whether a Metadata attribute is mutable in writes and
        updates.&nbsp; A typical example would be station properties
        like the location of the sensor, instrument response data,
        etc.&nbsp; Such parameters are expected to be read once by a
        reader and passed through a processing workflow until a write
        operation.&nbsp; They also are normally expected to be <a
          href="https://docs.mongodb.com/manual/core/data-model-design/">normalized</a>
        with the master copy in a separate collection from waveform
        data.&nbsp;&nbsp; <br>
      </li>
      <li>It is used by readers to sort out potentially ambiguous
        keys.&nbsp; A typical example would be instrument
        characteristics of a seismic observatory station.&nbsp;&nbsp;
        Sensors are changed, channel codes are changed, sensors can
        change orientation when swapped, etc.&nbsp;&nbsp; This can make
        critical metadata like response information time
        variable.&nbsp;&nbsp; (e.g. asking for the response data for
        station AAK channel BHZ is ambiguous for multiple
        reasons.)&nbsp;&nbsp; MetadataDefinitions was designed to
        abstract such information and front load the process of
        resolving such ambiguities to readers.&nbsp;&nbsp; More details
        on this interaction are given in the description (WILL NEED A
        LINK HERE) of the MongoDB python API.&nbsp;&nbsp;&nbsp;</li>
    </ol>
    For most users the practical issue is that most processing workflows
    will need to include these lines near the top of any python script:<br>
    <blockquote>from mspasspy import MetadataDefinitions<br>
      mdef=MetadataDefinitions()<br>
    </blockquote>
    This loads the default namespace.&nbsp;&nbsp; Alternatives are
    possible, but should be used only for specialized applications
    algorithms that require a different namespace.&nbsp; For example, in
    principle it should be possible to build a specialized configuration
    to build a MetadataDefinitions object that could be used to
    translate between the SAC or SEGY namespaces and mspass.&nbsp; <br>
    <br>
    A closely related object has the name <i>MongoDBConverter.&nbsp; </i>The
    <i>MongoDBConverter&nbsp;</i> caches a copy of the <i>MetadataDefinitions</i>
    it loads (usually behind the scenes).&nbsp; It has methods that
    provide an interface between the C++ objects and python that
    simplify database interactions with MongoDB. &nbsp; Most MongoDB
    CRUD operations functions require a&nbsp; <i>MongoDBConverter</i>
    as an argument. &nbsp; <br>
    <h4>Scalar versus 3C data</h4>
    <p>MsPASS currently supports two different data objects:&nbsp;&nbsp;
      TimeSeries is used to store single channel data while Seismogram
      is used to store data from three component instruments.&nbsp;
      TimeSeries objects are based on the standard concept for storing
      scalar data that has been around since the earliest days of
      digital seismic data in the oil and gas industry.&nbsp; That is,
      the sample values are stored in a continuous
      block of memory that we abstract as an array/vector.&nbsp;&nbsp;
      The index for the array serves as a proxy for time (<i>time </i>method
      in BasicTimeSeries).&nbsp;&nbsp; We use a C++ <a
        href="http://www.cplusplus.com/reference/vector/vector/">standard

        template library vector container</a> to hold the sample data
      accessible through the public variable s.&nbsp; The python API
      makes the vector container look like a numpy array that can be
      accessed in same way sample data are handled in an obspy Trace
      object in the "data" array.&nbsp;&nbsp; They can similarly be
      processed with the wide variety of operations available in scipy
      (e.g. <a
href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.iirfilter.html#scipy.signal.iirfilter">simple
        bandpass filters</a>).&nbsp; <br>
    </p>
    <p>Although scalar time series data are treated the same (i.e. as a
      vector) in every seismic processing system we are aware of, the
      handling of three component data is not at all
      standardized.&nbsp;&nbsp; There are several reasons for this
      created by some practical data issues: <br>
    </p>
    <ol>
      <li>&nbsp;Most modern seismic reflection systems provide some
        support for three-component data.&nbsp;&nbsp; In reflection
        processing scalar, multichannel raw data are often conceptually
        treated as a matrix with one array dimension defining the time
        variable and the other index defined by the channel number. When
        three component data are recorded the component orientation can
        be defined implicitly by a component index number.&nbsp;&nbsp; A
        3C shot gather than can be indexed conveniently with three array
        indexes.&nbsp; A complication in that approach is that which
        index is used for which of the three concept required for a
        gather of 3C data&nbsp; is completely undefined.&nbsp;&nbsp;
        Furthermore, for a generic system like mspass the multichannel
        model does not map cleanly into passive array data because a
        collection of 3C seismograms may have irregular size, may have
        variable sample rates,&nbsp; and may come from variable
        instrumentation.&nbsp; Hence, a simple matrix or array model
        would be very limiting.</li>
      <li>Traditional multichannel data have synchronous time
        sampling.&nbsp;&nbsp; Seismic reflection processing always
        assumes during processing that time computed from sample numbers
        is accurate to within one sample.&nbsp;&nbsp; Furthermore, the
        stock assumption is that all data have sample 0 at shot
        time;&nbsp; that assumption allows the conceptual model of a
        matrix to represent scalar, multichannel data.&nbsp; That is not
        necessarily true in passive array data and raw processing
        requires efforts to make sure the time of all samples can be
        computed accurately and time aligned.&nbsp; Alignment for a
        single stations is normally automatic although some instruments
        have measurable, constant phase lags at the single sample
        level.&nbsp; The bigger issue for all modern data is that the
        raw data are rarely stored in a multiplexed multichannel format,
        although the SEED format allows that. &nbsp; Most passive array
        data streams have multiple channels stored as compressed
        miniSEED packets that have to be unpacked and inserted into
        something like a vector container to be handled easily by a
        processing program.&nbsp;&nbsp; The process becomes more
        complicated for three-component data because at least three
        channels have to be manipulated and time aligned.&nbsp;&nbsp;
        The obspy package handles this issue by defining a Stream object
        that is a container of single channel Trace objects.&nbsp; They
        handle three component data as Stream objects with exactly three
        members in the container.&nbsp;&nbsp; <br>
      </li>
    </ol>
    We handle three component data in MsPASS by using a matrix, which we
    define with the symbol "u" following the convention in Aki and
    Richards, to store the data for a given <i>Seismogram</i>.&nbsp;&nbsp;
    There are two choices of the order of indices for this matrix.&nbsp;
    A <i>Seismogram</i> defines index 0(1) as the channel number and
    index 1(2) as the time index.&nbsp; The following python code
    section illustrates this more clearly than any words:<br>
    <blockquote>from mspasspy import Seismogram<br>
      d=Seismogram(100)&nbsp;&nbsp;&nbsp;&nbsp; # Create an empty
      Seismogram with storage for 100 time steps initialized to all
      zeros<br>
      d.u(0,50)=1.0
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

      # Create a delta function at time t0+dt*50 in channel 0<br>
    </blockquote>
    Note we use the C (an python) convention for indexing starting at
    0.&nbsp;&nbsp; In the C++ API the matrix u is defined with a
    lightweight implementation of a matrix as the data
    object.&nbsp;&nbsp; That detail is largely irrelevant to python
    programmers as the matrix is equivalenced to a numpy matrix by the
    wrappers.&nbsp;&nbsp; Hence, python programmers familiar with numpy
    can manipulate the data in the u matrix with all the tools of
    numpy.&nbsp; <br>
    <br>
    The Seismogram object has a minimal set of methods that the authors
    consider core concepts defining a three component seismogram.&nbsp;
    We limit these to coordinate transformations of the
    components.&nbsp;&nbsp; There are multiple methods for rotation of
    the components (overloaded rotate method), restoring data to
    cardinal directions at the instrument (rotate_to_standard),
    Kennett's free surface transformation, and a general transformation
    matrix.&nbsp;&nbsp; We use a pair of (public) boolean variables that
    are helpful for efficiency:&nbsp; <i>components_are_orthogonal</i>
    is true after any sequence of orthogonal transformations and <i>components_are_cardinal</i>
    is true when the components are in the standard ENZ directions.
    &nbsp; &nbsp; <br>
    <br>
    FIX BEFORE RELEASE:&nbsp;&nbsp; ENSEMBLE WRAPPERS HAVE NOT YET BEEN
    DEFINED OR TESTED<br>
    Ensembles of TimeSeries and Seismogram data are handled with a more
    elaborate standard template library container.&nbsp;&nbsp; For
    readers familiar with C++ the generic definition of an Ensemble is
    the following class definition created by stripping the comments
    from the definition in Ensemble.h:<br>
    <blockquote>template &lt;typename Tdata&gt; class Ensemble : public
      Metadata<br>
      {<br>
      public:<br>
      &nbsp; vector&lt;Tdata&gt; member;<br>
      &nbsp; ...<br>
      &nbsp; Tdata&amp; operator[](const int n) const<br>
      &nbsp; ...<br>
      }<br>
    </blockquote>
    where we omit all standard constuctors and methods to focus on the
    key issues here.&nbsp; First, an Ensemble is little more than a
    vector of data objects with a Metadata object to store attributes
    common to the entire ensemble.&nbsp; Hence, the idea is to store
    global attributes in the Ensemble Metadata field.&nbsp;&nbsp; There
    is a "dismember" algorithm in MsPASS (NOT YET IMPLEMENTED by already
    present in seispp and easy to implement) that takes this structure
    apart and copies the Metadata components into each member.&nbsp; The
    vector container makes it simple to handle an entire group
    (Ensemble) with a simple loop.&nbsp;&nbsp; e.g. here is a simple
    loop to work through an entire Ensemble (defined in this code
    segment with the symbol d) in order of the vector index:<br>
    <blockquote>n=d.member.size()<br>
      for i in range(n):<br>
      &nbsp; somefunction(d.member[i])&nbsp;&nbsp;&nbsp; # pass member i
      to somefunction<br>
    </blockquote>
    NOT SURE ABOUT THE ABOVE SYNTAX IN THE PYTHON API.&nbsp;&nbsp; COULD
    BE () INSTEAD OF []<br>
    <h4>MsPASSCoreTS&nbsp; and Core versus Top-level Data Objects<br>
    </h4>
    <p>The class hierarchy diagrams above show there are CoreTimeSeries
      and CoreSeismogram objects that are parents of TimeSeries and
      Seismogram respectively.&nbsp;&nbsp; That design was aimed to make
      the Core objects more readily extendible to other uses than
      MsPASS.&nbsp;&nbsp; We encourage users to consider using the core
      objects as base for other ways of handling seismic
      data.&nbsp;&nbsp; <br>
    </p>
    <p>All mspass specific elements of our implementation are in
      MsPASSCoreTS which is a parent for both TimeSeries and Seismogram
      objects.&nbsp;&nbsp; MsPASSCoreTS has two elements:<br>
    </p>
    <ol>
      <li>In MsPASS we use MongoDB for data management.&nbsp;&nbsp; In
        MongoDB the lowest common denominator to identify a particular
        "document" in the database is the <a
          href="https://docs.mongodb.com/manual/reference/method/ObjectId/">ObjectID</a>.&nbsp;

        We store a representation of the ObjectID that was used to
        create any data object read from the database.&nbsp;&nbsp; The <i>Metadata</i>
        object has a mechanism that keeps track of which attributes have
        been altered from the original. &nbsp; That feature can be
        exploited for pure <i>Metadata </i>operations to only update
        the changed attributes and retain the original data. &nbsp; When
        the sample data are altered the user is responsible for deciding
        if the original waveform data are to be retained and the new
        data added or updated in place. &nbsp; The ObjectID is critical
        for managing any update. &nbsp; <br>
      </li>
      <li>MsPASSCoreTS contains an error logging object.&nbsp;&nbsp; The
        purpose of this object is to contain a log of any errors or
        informative messages created during the processing of the
        data.&nbsp; All processing modules need to be designed with
        global error handlers so that they never abort, but in worst
        case post a log message that tags a fatal error.&nbsp;&nbsp;
        More details on this feature are given in the next section. <br>
      </li>
    </ol>
    <h4>Error Logging Concepts</h4>
    <p>When processing large volumes of data errors are inevitable and
      handling them clearly is an essential part of any processing
      framework.&nbsp;&nbsp; This is particularly challenging with a
      system like Spark where a data set gets fragmented and handled by
      (potentially) many processors.&nbsp;&nbsp; A poorly designed error
      handling system could abort an entire workflow if one function on
      one piece of data threw some kinds of "fatal" errors.&nbsp;&nbsp;
      <br>
    </p>
    <p>To handle this problem MsPASS uses a novel <i>ErrorLogger</i>
      object.&nbsp; Any data processing module in MsPASS should NEVER
      exit on any error condition except one from which the operating
      system cannot recover.&nbsp; All C++ and python processing modules
      need to have appropriate error handles (i.e. try/catch in C++ and
      try/except in python) to keep a single error from prematurely
      killing a large processing job. &nbsp; We recommend all error
      handlers in processing functions post a message that can help
      debug the error.&nbsp;&nbsp; Error messages should be registered
      with the data object's elog object.&nbsp;&nbsp; Error messages
      should not normally be just posted to stdout (i.e. print in
      python) for two reasons.&nbsp; First, stream io is not thread safe
      and garbled output is nearly guaranteed unless the log message are
      rare.&nbsp; Second, with a large dataset it can become a nearly
      impossible to find out which pieces of data created the
      errors.&nbsp; Proper application of the <i>ErrorLogger</i> object
      will eliminate both of these problems. <br>
    </p>
    <p>Multiple methods are available to post errors of severity from
      fatal to logging messages that do not necessarily indicate an
      error.&nbsp;&nbsp; A small python code segment may illustrate this
      more clearly.&nbsp;&nbsp; :<br>
    </p>
    <blockquote>try:<br>
      &nbsp; d.rotate_to_standard() <br>
      &nbsp; d.elog.log_verbose("rotate_to_standard succeed for me")<br>
      &nbsp; ...<br>
      except RuntimeError:<br>
      &nbsp; d.elog.log_error("rotate_to_standard method failure -
      transformation matrix may be singular",<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      ErrorSeverity.Invalid) <br>
      &nbsp; d.live=False&nbsp;&nbsp; # note in python just be False not
      false<br>
    </blockquote>
    To understand the code above assume the symbol d is a <i>Seismogram</i>
    object with a singular transformation matrix created, for example,
    by incorrectly building the object with two redundant east-west
    components.&nbsp;&nbsp; The rotate_to_standard method tries to
    compute a matrix inverse of the transformation matrix, which will
    generate an exception. &nbsp; This code catches that exception with
    a python RuntimeError.&nbsp; In this simple case we compose our own
    error message and post it to the <i>ErrorLogger</i> attached to
    this data (d.elog). &nbsp;The ErrorSeverity.Invalid implies the data
    are bad so the last line sets the live boolean false.&nbsp;&nbsp; In
    contrast, the call to log_verbose, like the name suggests, writes a
    pure informational message.&nbsp;&nbsp; <br>
    <br>
    All that would be usless baggage except the MongoDB database writers
    (Create and Update in CRUD) automatically save any elog entries in a
    separate database collection called elog.&nbsp;&nbsp; The saved
    messages can be linked back to the data with which they are
    associated through the ObjectID of the data in the wf
    collection.&nbsp; <br>
    <br>
  </body>
</html>
