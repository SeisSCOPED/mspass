'\" te
.TH DBVERIFY 1
#
.SH NAME
dbverify - MsPASS database verify command line tool
.SH SYNOPSIS
.nf
dbverify dbname [-t testname] [-c col ... ]
         [-n n1 ...] [-r k1 ...][-e n] [-v] [-h | --help]
.fi
.SH DESCRIPTION
.LP
\fIdbverify\fR is a command line tool to run a set of essential tests on
a dataset stored in the MsPASS database.   This program or the test
functions it uses should always be run on a dataset before starting
a long job to process data with MsPASS.  It performs essential validation
tests that are needed to reduce the chances of a large workflow aborting
during processing.  The command line tool, \fIdbverify\fR, should be viewed as a
front-end to test functions in the python library of MsPASS.  This tool
was written largely to provide a way simply the output of the test
functions and produce output that can be more easily understood. Note the
tool was intentionally designed to run only one test at a time to
simplify output.  Because MongoDB supports many simultaneous clients to
test large data sets it may be useful to run each test as a separate job.

The following tests are currently supported.
They can only be run one at a time and are controlled by the \fItestname\fR
argument attached to the -t flag:
.IP (1)
\fInormalization\fR:  This test validates cross-references to collections that
MongoDB calls normalized data.  Normalized data are smaller collections of
common data that are more efficient to store together and link to the
waveform data through a query with a cross-referencing key.   In MsPASS
current normalization collections in the standard schema are called
\fIsite, channel\fR, and \fIsource\fR.  When this test is run the -n
option can be used to control what normalization collections are tested.
The default is equivalent to "-n site".   The collection on which the
test is to be run can be set with the -c option.  For this test -c should
reference either wf_TimeSeries or wf_Seismogram.  It can only be run on
one collection at a time.
.IP (2)
\fIrequired\fR:   Certain attributes are always required for processing.
Use the -r option to provide a list of keys that should be
checked for existence in the specified collection.   If -r is not give
an internal default is used.   We recommend always providing a list of
what your workflow considers required.  Only one collection can be
processed at a time to keep the -r list manageable.
.IP (3)
\fIschema_check\fR:   Although MongoDB is completely promiscuous about
types of attributes associated with any key, in MsPASS we imposed some order on the
system through a schema.   Some attributes can have type requirements that
may cause a job to abort if they are improperly defined.  The idea of this
test is to flag all possible key-value pairs in a collection that
are inconsistent with the schema.  Some mismatches are mostly harmless
while others are nearly guaranteed to cause processing problems.
What defines "mostly harmless" is discussed in more detail below in the
section titled "Interpreting Test Results".   This is currently the only
test that can be run on multiple collections in a single run.   The reason
is it depends completely on the schema and needs no other direction
other than a list of collections to scan.

.SH ARGUMENTS
.LP
Argument 1 is require to be a valid database name.   The program will attempt
to connect the MongoDB server and access the data with that name tag.  The
program will abort immediately if MongoDB is not running.   If you give an
invalid name you can expect mysterious errors - most likely complaints that
the collection you tried to test has no data.
.IP -t
Control the testname as described above.  Can also use --test.
.IP -c
This flag is used to mark the list of one or more collections.  Only the
\fIschema_check\fR test supports multiple collections in one run. The other tests have
argument dependencies that do not map well to the command line argument
method of control parameter inputs.  For those tests if multiple arguments
follow -c only the first will be used and a warning error will be printed.
Can also use --collection as this flag.
.IP -n
This is used to mark the start of a list of secondary collections to that
defined by -c that are expected to resolve with cross reference ids
in the \fInormalization\fR test.   Any -n arguments for
tests other than \fInormalization\fR will be ignored.  Can also use --normalize.
.IP -r
This is used to mark a list of required key attributes for the \frequired\fR
test.   Any arguments after -r will be ignored by other tests.
Can also use --required.
.IP -v
Runs in verbose mode.  Default is to print only a summary of the test results.
Verbose mode is most useful as a followup when errors are detected in one
or more collections and you need more information about what is wrong.
Can also use --verbose
.IP -e
Sets a limit on the number of errors found before stopping the test.
A large dataset with a common error in most or all the documents in
a collection can generate voluminous output in verbose mode and pass the
point of diminishing returns for the standard summary mode. The default is
1000.  Smaller numbers are often useful in verbose mode and larger numbers
may be needed for data sets assembled from multiple sources with different
problems.
.SH EXAMPLES
.nf
# Run the normalization test on the wf_Seismogram collection:
dbverify mydb -c wf_Seismogram -t normalization -n site source
# Run the required test on the channel collection
dbverify mydb -c channel -t required -r lat lon elev hang vang starttime endtime
# Run the schema_check test on all standard collections fpr TimeSeries data
dbverify mydb -c wf_TimeSeries channel source -t schema_check
# Rerun schema test on wf_TimeSeries to clarify errors
# impose a limit of only 100 documents to keep the output reasonable.
dbverify mydb -c wf_TimeSeries -t schema_check -v -e 100
.fi
.SH INTERPRETING TEST RESULTS
We recommend this program should usually be run in the default nonverbose mode.
In that mode all the tests attempt to simply summarize the errors the test
found.  In most cases that is list of offending keys and the number of documents
found with the particular error.

The \fInormalization\fR test print an all is well message if it finds
no errors for a given collection.  If there are problems it will simply
list the number of documents it found with an error.  If that number is
the same as the error limit or the number of waveforms in the database it
probably means all links to that collection are broken.

In normal mode the \fIschema_check\fR and \fIrequired\fR test both produce
an output of offending keys and the number of documents they found with
that error.  The types of errors are defined in the print statements.

In all cases, the way you should use this program is to first always run
it in the default (not verbose) mode.  If any of the test fail look
at the output carefully.  Some errors may not require rerunning the
program.  For example, keys that are not defined in the schema will
generate errors in multiple tests. Undefined keys are usually harmless
but they do waste storage if they aren't really needed.  Type collisions
can be the hardest to sort out.  If you have type mismatch errors
we recommend the first step is to rerun the same test in verbose mode
to look at the full set of attributes stored in the offending collection
for a few errors.  As noted earlier when running in verbose mode it is
usually prudent to reduce the error limit size with the -e option.
Many type errors are recoverable and some are not.  For example, it is
relatively easy in python to unintentionally post a real number as an integer.
That is particularly so when data are entered as literals in a python script.
(e.g. in python the statements x=11.0 and y=11 do not the same things even though
the numbers might seem the same.  x is a float and y is an integer.)
The MsPASS readers will convert type collisions like the example above
automatically.  What is not repairable are string data appearing in a field
the schema requires to be a number.  e.g. if one posted lat='37N22.40' it might
look right but cannot be automatically converted.  The point is type
mismatches can be hard to detect without this tool and even after it is
detected it can be a challenge to see the problem from the output.
Fortunately, such errors are rare unless a function has a bug or the input
is created by some simulation method that doesn't set some attributes
correctly. 

.SH "SEE ALSO"
.LP
The tests this program runs are python functions found in the module
mspasspy.db.verify.   See the sphynx documentation for more about
the individual functions.
.SH "BUGS AND CAVEATS"
.LP
The function currently does not support any query mechanism on a
collection it processes.  It will only work on an entire collection or
set of collections you direct the program to process.  All the current
tests are reasonably fast and should be feasible as a serial process
for any currently conceivable dataset, but for very large data sets
some of them could take some time to complete.   If you need to
query a smaller data volume use the python functions directly.
.SH AUTHOR
.nf
Gary L. Pavlis
Department of Earth and Atmospheric Sciences
Indiana University
.fi
